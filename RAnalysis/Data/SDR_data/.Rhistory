plot(as.numeric(as.character(TMP_T0)) ~ Date.Time, Probe.Data, col = "grey", type="l", ylim=c(12, 20),  xlab="Time", ylab="Temperature Â°C")
lines(as.numeric(as.character(TMP_T1)) ~ Date.Time, Probe.Data, col = "red")
lines(as.numeric(as.character(TMP_T2)) ~ Date.Time, Probe.Data, col = "blue")
lines(as.numeric(as.character(Temp_T3)) ~ Date.Time, Probe.Data, col = "black")
axis.POSIXct(side=1, Probe.Data$Date.Time)
plot(as.numeric(as.character(pH_T0)) ~ Date.Time, Probe.Data, col = "grey", type="l", ylim=c(7.1, 8.1),  xlab="Time", ylab="pH NBS")
lines(as.numeric(as.character(pH_T1)) ~ Date.Time, Probe.Data, col = "red")
lines(as.numeric(as.character(pH_T2)) ~ Date.Time, Probe.Data, col = "blue")
lines(as.numeric(as.character(pH_T3)) ~ Date.Time, Probe.Data, col = "black")
axis.POSIXct(side=1, Probe.Data$Date.Time)
# plot(as.numeric(as.character(Salt_XL)) ~ Date.Time, Probe.Data, col = "grey", type="l", ylim=c(20, 35),  xlab="Time", ylab="Salinity psu")
# lines(as.numeric(as.character(Salt_L)) ~ Date.Time, Probe.Data, col = "red")
# lines(as.numeric(as.character(Salt_A)) ~ Date.Time, Probe.Data, col = "blue")
# axis.POSIXct(side=1, Probe.Data$Date.Time)
dev.off()
?LoLinR
resp<-read.table(file.choose(), header=T, sep=",")
resp
library(LoLinR)
data(BugulaData)
BugalaRegs<-rankLocReg(
xall-resp$Date.Time,yall=resp$A1,
alpha=0.2, method="eq")
BugalaRegs<-rankLocReg(
xall=resp$Date.Time,yall=resp$A1,
alpha=0.2, method="eq")
BugalaRegs<-rankLocReg(
xall=resp$Date.Time,yall=resp$A1,
alpha=0.2, method="eq")
BugalaRegs<-rankLocReg(
xall=resp$Date.Time,yall=resp$A1,
alpha=0.2, method="eq")
BugalaRegs<-rankLocReg(
xall=resp$Date.Time,yall=resp$A1,
alpha=0.2, method="eq")
BugalaRegs
summary(BugalaRegs)
resp_H0_T<-rankLocReg(
xall=resp$Date.Time,yall=resp$A1,
alpha=0.2, method="eq")
resp_H0_T<-rankLocReg(
xall=resp$Date.Time,yall=resp$A1,
alpha=0.2, method="eq")
resp_H0_T
summary(resp_H0_T)
resp_H0_T<-rankLocReg(
xall=resp$Date.Time,yall=resp$A1,
alpha=0.2, method="eq")
resp_H0_T<-rankLocReg(
xall=resp$Date.Time,yall=resp$A1,
alpha=0.2, method="eq")
resp_H0_T<-rankLocReg(
xall=resp$Date.Time,yall=resp$A1,
alpha=0.2, method="eq")
resp_H0_T
summary(resp_H0_T)
resp_H1_T <-rankLocReg(
xall=resp$Date.Time,yall=resp$A4,
alpha=0.2, method="eq")
summary(resp_H1_T)
resp_H0_B<-rankLocReg(
xall=resp$Date.Time,yall=resp$B1,
alpha=0.2, method="eq")
summary(resp_H0_B)
resp_H1_B<-rankLocReg(
xall=resp$Date.Time,yall=resp$B4,
alpha=0.2, method="eq")
summary(resp_H1_B)
library("XML")
library("plyr")
xmlfile <- xmlParse("http://192.168.1.100:80/cgi-bin/datalog.xml?sdate=180724&days=9") #read in the date plus x days of Apex data
Apex.Data <- ldply(xmlToList(xmlfile), data.frame) #convert xml to dataframe
Apex.Data2 <- Apex.Data[4:nrow(Apex.Data),] #remove extra metadata from top
Apex.Data2 <- head(Apex.Data2,-2) #remove extra metadata from bottom
# view data and names to ID the raw probe.name or probe.type or probe.value
Apex.Data2
names(Apex.Data2)
#keep columnes with data of interest. This needs to be changed as it will be specific to the Apex configuration
Probe.Data <- Apex.Data2[,c(3,6,9,12,66, 69, 72, 75, 78, 81)] #select columns
colnames(Probe.Data ) <- c("Date.Time", "TMP_T0", "pH_T0", "Sal", "TMP_T2", "pH_T2","Temp_T3",
"pH_T3", "TMP_T1", "pH_T1")  #rename columns
Probe.Data$Date.Time <- as.POSIXct(Probe.Data$Date.Time, format = "%m/%d/%Y %H:%M:%S", tz="HST") #convert date to HI time
# CHANGE DATE FOR NEW CSV (risk overwritting previous)
write.csv(Probe.Data, "C:/Users/samjg/Documents/Notebook/data/Geoduck_Conditioning/RAnalysis/Data/Apex_data/Output/20180801_Apex_Data_Output.data.csv") #write file to save data
#plot Temp and pH and save to output
# CHANGE DATE FOR NEW PDF (risk overwritting previous)
pdf("C:/Users/samjg/Documents/Notebook/data/Geoduck_Conditioning/RAnalysis/Data/Apex_data/Output/Graphs/20180801_Apex_Output.data.pdf")
par(mfrow=c(2,1))
plot(as.numeric(as.character(TMP_T0)) ~ Date.Time, Probe.Data, col = "grey", type="l", ylim=c(12, 20),  xlab="Time", ylab="Temperature Â°C")
lines(as.numeric(as.character(TMP_T1)) ~ Date.Time, Probe.Data, col = "red")
lines(as.numeric(as.character(TMP_T2)) ~ Date.Time, Probe.Data, col = "blue")
lines(as.numeric(as.character(Temp_T3)) ~ Date.Time, Probe.Data, col = "black")
axis.POSIXct(side=1, Probe.Data$Date.Time)
plot(as.numeric(as.character(pH_T0)) ~ Date.Time, Probe.Data, col = "grey", type="l", ylim=c(7.1, 8.1),  xlab="Time", ylab="pH NBS")
lines(as.numeric(as.character(pH_T1)) ~ Date.Time, Probe.Data, col = "red")
lines(as.numeric(as.character(pH_T2)) ~ Date.Time, Probe.Data, col = "blue")
lines(as.numeric(as.character(pH_T3)) ~ Date.Time, Probe.Data, col = "black")
axis.POSIXct(side=1, Probe.Data$Date.Time)
# plot(as.numeric(as.character(Salt_XL)) ~ Date.Time, Probe.Data, col = "grey", type="l", ylim=c(20, 35),  xlab="Time", ylab="Salinity psu")
# lines(as.numeric(as.character(Salt_L)) ~ Date.Time, Probe.Data, col = "red")
# lines(as.numeric(as.character(Salt_A)) ~ Date.Time, Probe.Data, col = "blue")
# axis.POSIXct(side=1, Probe.Data$Date.Time)
dev.off()
#http://www.informit.com/articles/article.aspx?p=2215520
#modified as of 20180715 SJG
#added reminders on lines 39 and 43 to prevent overwritting files ->  SJG
#changes in lines 20 - 36 for column name changes for switched conicals -> 20170708 by SJG
library("XML")
library("plyr")
xmlfile <- xmlParse("http://192.168.1.100:80/cgi-bin/datalog.xml?sdate=180801&days=5") #read in the date plus x days of Apex data
Apex.Data <- ldply(xmlToList(xmlfile), data.frame) #convert xml to dataframe
Apex.Data2 <- Apex.Data[4:nrow(Apex.Data),] #remove extra metadata from top
Apex.Data2 <- head(Apex.Data2,-2) #remove extra metadata from bottom
# view data and names to ID the raw probe.name or probe.type or probe.value
Apex.Data2
names(Apex.Data2)
# Column names modified as of 20180707
# Date.Time = column 2
# TMP_T0 = column 6
# pH_T0= column 9
# salinity = column 12
# TMP_T2 = column 66
# pH_T2 = column 69
# TMP_T3 = column 72
# pH_T3 = column 75
# TMP_T1 = column 78
# pH_T1 = column 81
# NOTE: 10 in total above
#keep columnes with data of interest. This needs to be changed as it will be specific to the Apex configuration
Probe.Data <- Apex.Data2[,c(3,6,9,12,66, 69, 72, 75, 78, 81)] #select columns
colnames(Probe.Data ) <- c("Date.Time", "TMP_T0", "pH_T0", "Sal", "TMP_T2", "pH_T2","Temp_T3",
"pH_T3", "TMP_T1", "pH_T1")  #rename columns
Probe.Data$Date.Time <- as.POSIXct(Probe.Data$Date.Time, format = "%m/%d/%Y %H:%M:%S", tz="HST") #convert date to HI time
# CHANGE DATE FOR NEW CSV (risk overwritting previous)
write.csv(Probe.Data, "C:/Users/samjg/Documents/Notebook/data/Geoduck_Conditioning/RAnalysis/Data/Apex_data/Output/20180805_Apex_Data_Output.data.csv") #write file to save data
#plot Temp and pH and save to output
# CHANGE DATE FOR NEW PDF (risk overwritting previous)
pdf("C:/Users/samjg/Documents/Notebook/data/Geoduck_Conditioning/RAnalysis/Data/Apex_data/Output/Graphs/20180805_Apex_Output.data.pdf")
par(mfrow=c(2,1))
plot(as.numeric(as.character(TMP_T0)) ~ Date.Time, Probe.Data, col = "grey", type="l", ylim=c(12, 20),  xlab="Time", ylab="Temperature Â°C")
lines(as.numeric(as.character(TMP_T1)) ~ Date.Time, Probe.Data, col = "red")
lines(as.numeric(as.character(TMP_T2)) ~ Date.Time, Probe.Data, col = "blue")
lines(as.numeric(as.character(Temp_T3)) ~ Date.Time, Probe.Data, col = "black")
axis.POSIXct(side=1, Probe.Data$Date.Time)
plot(as.numeric(as.character(pH_T0)) ~ Date.Time, Probe.Data, col = "grey", type="l", ylim=c(7.1, 8.1),  xlab="Time", ylab="pH NBS")
lines(as.numeric(as.character(pH_T1)) ~ Date.Time, Probe.Data, col = "red")
lines(as.numeric(as.character(pH_T2)) ~ Date.Time, Probe.Data, col = "blue")
lines(as.numeric(as.character(pH_T3)) ~ Date.Time, Probe.Data, col = "black")
axis.POSIXct(side=1, Probe.Data$Date.Time)
# plot(as.numeric(as.character(Salt_XL)) ~ Date.Time, Probe.Data, col = "grey", type="l", ylim=c(20, 35),  xlab="Time", ylab="Salinity psu")
# lines(as.numeric(as.character(Salt_L)) ~ Date.Time, Probe.Data, col = "red")
# lines(as.numeric(as.character(Salt_A)) ~ Date.Time, Probe.Data, col = "blue")
# axis.POSIXct(side=1, Probe.Data$Date.Time)
dev.off()
library("XML")
library("plyr")
xmlfile <- xmlParse("http://192.168.1.100:80/cgi-bin/datalog.xml?sdate=1800805&days=10") #read in the date plus x days of Apex data
Apex.Data2 <- Apex.Data[4:nrow(Apex.Data),] #remove extra metadata from top
Apex.Data <- ldply(xmlToList(xmlfile), data.frame) #convert xml to dataframe
Apex.Data2 <- head(Apex.Data2,-2) #remove extra metadata from bottom
# view data and names to ID the raw probe.name or probe.type or probe.value
Apex.Data2
names(Apex.Data2)
library("XML")
library("plyr")
xmlfile <- xmlParse("http://192.168.1.100:80/cgi-bin/datalog.xml?sdate=1800805&days=10") #read in the date plus x days of Apex data
Apex.Data <- ldply(xmlToList(xmlfile), data.frame) #convert xml to dataframe
Apex.Data2 <- Apex.Data[4:nrow(Apex.Data),] #remove extra metadata from top
Apex.Data2 <- head(Apex.Data2,-2) #remove extra metadata from bottom
# view data and names to ID the raw probe.name or probe.type or probe.value
Apex.Data2
names(Apex.Data2)
#keep columnes with data of interest. This needs to be changed as it will be specific to the Apex configuration
Probe.Data <- Apex.Data2[,c(3,6,9,12,66, 69, 72, 75, 78, 81)] #select columns
xmlfile <- xmlParse("http://192.168.1.100:80/cgi-bin/datalog.xml?sdate=1800805&days=10") #read in the date plus x days of Apex data
xmlfile <- xmlParse("http://192.168.1.100:80/cgi-bin/datalog.xml?sdate=180805&days=10") #read in the date plus x days of Apex data
Apex.Data <- ldply(xmlToList(xmlfile), data.frame) #convert xml to dataframe
Apex.Data2 <- Apex.Data[4:nrow(Apex.Data),] #remove extra metadata from top
Apex.Data2 <- head(Apex.Data2,-2) #remove extra metadata from bottom
# view data and names to ID the raw probe.name or probe.type or probe.value
Apex.Data2
names(Apex.Data2)
#keep columnes with data of interest. This needs to be changed as it will be specific to the Apex configuration
Probe.Data <- Apex.Data2[,c(3,6,9,12,66, 69, 72, 75, 78, 81)] #select columns
colnames(Probe.Data ) <- c("Date.Time", "TMP_T0", "pH_T0", "Sal", "TMP_T2", "pH_T2","Temp_T3",
"pH_T3", "TMP_T1", "pH_T1")  #rename columns
Probe.Data$Date.Time <- as.POSIXct(Probe.Data$Date.Time, format = "%m/%d/%Y %H:%M:%S", tz="HST") #convert date to HI time
# CHANGE DATE FOR NEW CSV (risk overwritting previous)
write.csv(Probe.Data, "C:/Users/samjg/Documents/Notebook/data/Geoduck_Conditioning/RAnalysis/Data/Apex_data/Output/20180814_Apex_Data_Output.data.csv") #write file to save data
#plot Temp and pH and save to output
# CHANGE DATE FOR NEW PDF (risk overwritting previous)
pdf("C:/Users/samjg/Documents/Notebook/data/Geoduck_Conditioning/RAnalysis/Data/Apex_data/Output/Graphs/20180814_Apex_Output.data.pdf")
par(mfrow=c(2,1))
plot(as.numeric(as.character(TMP_T0)) ~ Date.Time, Probe.Data, col = "grey", type="l", ylim=c(12, 20),  xlab="Time", ylab="Temperature Â°C")
lines(as.numeric(as.character(TMP_T1)) ~ Date.Time, Probe.Data, col = "red")
lines(as.numeric(as.character(TMP_T2)) ~ Date.Time, Probe.Data, col = "blue")
lines(as.numeric(as.character(Temp_T3)) ~ Date.Time, Probe.Data, col = "black")
axis.POSIXct(side=1, Probe.Data$Date.Time)
plot(as.numeric(as.character(pH_T0)) ~ Date.Time, Probe.Data, col = "grey", type="l", ylim=c(7.1, 8.1),  xlab="Time", ylab="pH NBS")
lines(as.numeric(as.character(pH_T1)) ~ Date.Time, Probe.Data, col = "red")
lines(as.numeric(as.character(pH_T2)) ~ Date.Time, Probe.Data, col = "blue")
lines(as.numeric(as.character(pH_T3)) ~ Date.Time, Probe.Data, col = "black")
axis.POSIXct(side=1, Probe.Data$Date.Time)
# plot(as.numeric(as.character(Salt_XL)) ~ Date.Time, Probe.Data, col = "grey", type="l", ylim=c(20, 35),  xlab="Time", ylab="Salinity psu")
# lines(as.numeric(as.character(Salt_L)) ~ Date.Time, Probe.Data, col = "red")
# lines(as.numeric(as.character(Salt_A)) ~ Date.Time, Probe.Data, col = "blue")
# axis.POSIXct(side=1, Probe.Data$Date.Time)
dev.off()
# Assumption: raw SDR ouput files are edited for headers (row1) in vial order A1....D1, A2, ...D2, etc.
####################################################
rm(list=ls())
#set working directory--------------------------------------------------------------------------------------------------
setwd("C:/Users/samjg/Documents/Notebook/data/Geoduck_Conditioning/RAnalysis/Data/SDR_data")
main<-getwd()
#LIBRARY-----------------------------------------------------------------------------------------------------------
library(LoLinR)
# path used in the loop to each file name indicated in Table_files (matrix of filenames created below)
path<-"All_data_csv" #the location of all your respiration files
# assign your alpha value
a <- 0.4 #---------------------------------------------------------------------CHANGE THE NAME HERE EVERY TIME ( AND in the model itself!)
#assign your "Notes" column - refer to the amount of data used
n <- "all" #---------------------------------------------------------------CHANGE THE NAME HERE EVERY TIME
# time gap for assigned resp vials
# write this for minutes 10-25 resp_vials[40:100,] min10-25_2
# write this for minutes 10-20 resp_vials[40:80,] min10-20_2
ouputNAME<-"Cumulative_resp_alpha0.4_all_2.csv" # -------------------------CHANGE THE NAME HERE EVERY TIME
# size data to merge to output
path2<-"All_data_csv/Cumulative_output"
size.file<-"/sampleID_number_size.csv"
#load Data------------------------------------------------------------------------------------------
size<-read.csv(file.path(path2,size.file), header=T, sep=",", na.string="NA", as.is=T)
# combine a common ID to merge size with resp
size$Sample.ID <- paste(size$Date, size$SDR_position, size$RUN, sep="_")
#omit the rows that you just merged to avoid redundant data in the final output
size$RUN <- NULL
size$Date<-NULL
size$SDR_position<- NULL
# make a table for to refer in the outer for loop - filenames, run, and date
TABLE_files <- data.frame(matrix(nrow = 16))
TABLE_files$run_number <- c(1,2,1,2,1,2,1,2,1,2,1,2,1,2,1,2)
TABLE_files$date <- c("20180716", "20180716", "20180719", "20180719", "20180722", "20180722",
"20180724", "20180724", "20180807", "20180807", "20180809", "20180809",
"20180811", "20180811", "20180813", "20180813")
# each file name for the outer for loop
TABLE_files$filenames <- c(
"20180716_resp_Day2_RUN1_Oxygen.csv",
"20180716_resp_Day2_RUN2_Oxygen.csv",
"20180719_resp_Day5_RUN1_Oxygen.csv",
"20180719_resp_Day5_RUN2_Oxygen.csv",
"20180722_resp_Day8_RUN1_Oxygen.csv",
"20180722_resp_Day8_RUN2_Oxygen.csv",
"20180724_resp_Day10_RUN1_Oxygen.csv",
"20180724_resp_Day10_RUN2_Oxygen.csv",
"20180807_resp_Day0_RUN1_Oxygen.csv",
"20180807_resp_Day0_RUN2_Oxygen.csv",
"20180809_resp_Day2_RUN1_Oxygen.csv",
"20180809_resp_Day2_RUN2_Oxygen.csv",
"20180811_resp_Day4_RUN1_Oxygen.csv",
"20180811_resp_Day4_RUN2_Oxygen.csv",
"20180813_resp_Day6_RUN1_Oxygen.csv",
"20180813_resp_Day6_RUN2_Oxygen.csv")
df_total = data.frame()
# FOR loop calls from LoLin, models sequencially for each ind.var column,
# calls Lpc from summary table, writes to a dataframe (24 rows / file - 1 row populated per column)
for(i in 1:nrow(TABLE_files)){
resp<-read.csv(file.path("All_data_csv", (TABLE_files[i,4])), header=T, sep=",", na.string="NA", as.is=T)
# omit the Date.Time from the dataframe
resp$Date.Time <-NULL
Date <- TABLE_files[i,3]
RUN <- TABLE_files[i,2]
# your predicted values
resp_vials <- resp[, !(colnames(resp) %in% c("Time.Min."))]
resp_vials
# inside for loop calls the 24 columns individually for the LoLin model
# ouputs the Lpc individually to the created dataframe and rbinds it to df_total
for(j in t(1:ncol(resp_vials))){
model <- rankLocReg(
xall=resp$Time.Min., yall=resp_vials[, j],
alpha=0.4, method="pc", verbose=TRUE)
sum.table<-summary(model)
resp.table <- data.frame(matrix(nrow = 1, ncol = 6))
colnames(resp.table)<-c('Date', 'RUN', 'SDR_position', 'b1.Lpc', 'alpha', 'Notes')
resp.table$Date <- Date
resp.table$RUN <- RUN
resp.table$alpha <- a
resp.table$Notes <- n
#take data from your model summary (example is the slope and range of confidence interval)
#resp.table$b1.Lz<-sum.table$Lcompare[1,6]
#resp.table$b1.Leq<-sum.table$Lcompare[2,6]
resp.table$b1.Lpc<-sum.table$Lcompare[3,6]
#resp.table$ci.Lz<-sum.table$Lcompare[1,9]
#resp.table$ci.Leq<-sum.table$Lcompare[2,9]
#resp.table$ci.Lpc<-sum.table$Lcompare[3,9]
#name dataframe for this single row
df <- data.frame(resp.table)
#bind to a cumulative list dataframe
df_total <- rbind(df_total,df)
print(df_total)
}
}
#look at your cumulative dataframe
df_total
#names from a1 to D6 in order of default in SDR sensor dish - X16 for the 16 files
names <- c("A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6")
# name the SDR position in the cumulative dataframe to merge with size
df_total$SDR_position <- names
# create a column in the cumulative dataframe that matches the size dataframe
# generate new row with concatenated sample id
df_total$Sample.ID <- paste(df_total$Date, df_total$SDR_position, df_total$RUN, sep='_')
# merge the cumulative dataframe from the loop with the size dataframe
update.data <- merge(df_total,size, by="Sample.ID", all = TRUE, sort = T)
# write out to the path names outputNAME
write.table(update.data,ouputNAME,sep=",", row.names=FALSE)
# Assumption: raw SDR ouput files are edited for headers (row1) in vial order A1....D1, A2, ...D2, etc.
####################################################
rm(list=ls())
#set working directory--------------------------------------------------------------------------------------------------
setwd("C:/Users/samjg/Documents/Notebook/data/Geoduck_Conditioning/RAnalysis/Data/SDR_data")
main<-getwd()
#LIBRARY-----------------------------------------------------------------------------------------------------------
library(LoLinR)
# path used in the loop to each file name indicated in Table_files (matrix of filenames created below)
path<-"All_data_csv" #the location of all your respiration files
# assign your alpha value
a <- 0.6 #---------------------------------------------------------------------CHANGE THE NAME HERE EVERY TIME ( AND in the model itself!)
#assign your "Notes" column - refer to the amount of data used
n <- "all" #---------------------------------------------------------------CHANGE THE NAME HERE EVERY TIME
# time gap for assigned resp vials
# write this for minutes 10-25 resp_vials[40:100,] min10-25_2
# write this for minutes 10-20 resp_vials[40:80,] min10-20_2
ouputNAME<-"Cumulative_resp_alpha0.6_all_2.csv" # -------------------------CHANGE THE NAME HERE EVERY TIME
# size data to merge to output
path2<-"All_data_csv/Cumulative_output"
size.file<-"/sampleID_number_size.csv"
#load Data------------------------------------------------------------------------------------------
size<-read.csv(file.path(path2,size.file), header=T, sep=",", na.string="NA", as.is=T)
# combine a common ID to merge size with resp
size$Sample.ID <- paste(size$Date, size$SDR_position, size$RUN, sep="_")
#omit the rows that you just merged to avoid redundant data in the final output
size$RUN <- NULL
size$Date<-NULL
size$SDR_position<- NULL
# make a table for to refer in the outer for loop - filenames, run, and date
TABLE_files <- data.frame(matrix(nrow = 16))
TABLE_files$run_number <- c(1,2,1,2,1,2,1,2,1,2,1,2,1,2,1,2)
TABLE_files$date <- c("20180716", "20180716", "20180719", "20180719", "20180722", "20180722",
"20180724", "20180724", "20180807", "20180807", "20180809", "20180809",
"20180811", "20180811", "20180813", "20180813")
# each file name for the outer for loop
TABLE_files$filenames <- c(
"20180716_resp_Day2_RUN1_Oxygen.csv",
"20180716_resp_Day2_RUN2_Oxygen.csv",
"20180719_resp_Day5_RUN1_Oxygen.csv",
"20180719_resp_Day5_RUN2_Oxygen.csv",
"20180722_resp_Day8_RUN1_Oxygen.csv",
"20180722_resp_Day8_RUN2_Oxygen.csv",
"20180724_resp_Day10_RUN1_Oxygen.csv",
"20180724_resp_Day10_RUN2_Oxygen.csv",
"20180807_resp_Day0_RUN1_Oxygen.csv",
"20180807_resp_Day0_RUN2_Oxygen.csv",
"20180809_resp_Day2_RUN1_Oxygen.csv",
"20180809_resp_Day2_RUN2_Oxygen.csv",
"20180811_resp_Day4_RUN1_Oxygen.csv",
"20180811_resp_Day4_RUN2_Oxygen.csv",
"20180813_resp_Day6_RUN1_Oxygen.csv",
"20180813_resp_Day6_RUN2_Oxygen.csv")
df_total = data.frame()
# FOR loop calls from LoLin, models sequencially for each ind.var column,
# calls Lpc from summary table, writes to a dataframe (24 rows / file - 1 row populated per column)
for(i in 1:nrow(TABLE_files)){
resp<-read.csv(file.path("All_data_csv", (TABLE_files[i,4])), header=T, sep=",", na.string="NA", as.is=T)
# omit the Date.Time from the dataframe
resp$Date.Time <-NULL
Date <- TABLE_files[i,3]
RUN <- TABLE_files[i,2]
# your predicted values
resp_vials <- resp[, !(colnames(resp) %in% c("Time.Min."))]
resp_vials
# inside for loop calls the 24 columns individually for the LoLin model
# ouputs the Lpc individually to the created dataframe and rbinds it to df_total
for(j in t(1:ncol(resp_vials))){
model <- rankLocReg(
xall=resp$Time.Min., yall=resp_vials[, j],
alpha=0.6, method="pc", verbose=TRUE)
sum.table<-summary(model)
resp.table <- data.frame(matrix(nrow = 1, ncol = 6))
colnames(resp.table)<-c('Date', 'RUN', 'SDR_position', 'b1.Lpc', 'alpha', 'Notes')
resp.table$Date <- Date
resp.table$RUN <- RUN
resp.table$alpha <- a
resp.table$Notes <- n
#take data from your model summary (example is the slope and range of confidence interval)
#resp.table$b1.Lz<-sum.table$Lcompare[1,6]
#resp.table$b1.Leq<-sum.table$Lcompare[2,6]
resp.table$b1.Lpc<-sum.table$Lcompare[3,6]
#resp.table$ci.Lz<-sum.table$Lcompare[1,9]
#resp.table$ci.Leq<-sum.table$Lcompare[2,9]
#resp.table$ci.Lpc<-sum.table$Lcompare[3,9]
#name dataframe for this single row
df <- data.frame(resp.table)
#bind to a cumulative list dataframe
df_total <- rbind(df_total,df)
print(df_total)
}
}
#look at your cumulative dataframe
df_total
#names from a1 to D6 in order of default in SDR sensor dish - X16 for the 16 files
names <- c("A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6",
"A1", "B1", "C1", "D1", "A2", "B2", "C2", "D2",
"A3", "B3", "C3", "D3", "A4", "B4" ,"C4", "D4",
"A5", "B5", "C5", "D5", "A6", "B6", "C6", "D6")
# name the SDR position in the cumulative dataframe to merge with size
df_total$SDR_position <- names
# create a column in the cumulative dataframe that matches the size dataframe
# generate new row with concatenated sample id
df_total$Sample.ID <- paste(df_total$Date, df_total$SDR_position, df_total$RUN, sep='_')
# merge the cumulative dataframe from the loop with the size dataframe
update.data <- merge(df_total,size, by="Sample.ID", all = TRUE, sort = T)
# write out to the path names outputNAME
write.table(update.data,ouputNAME,sep=",", row.names=FALSE)
